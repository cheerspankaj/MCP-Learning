{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to OpenAI API & Prompting\n",
    "\n",
    "This module introduces participants to the motivation for working with the OpenAI API and the fundamentals for doing so. Participants will learn about the OpenAI Platform, types of prompting, and be introduced to advanced topics covered in later modules (e.g. RAG, Agents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT -> API \n",
    "Let's begin by exploring how we could use ChatGPT for a data validation task and compare the responses from different models.\n",
    "\n",
    "Below we have some sample data for concommitant medications and medical history events. Let's ask two different models in ChatGPT to identify medications that do not have an associated condition (a likely data quality issue). The optimal answer is that the model identifies that Advil (Patient 1) and Creatine (Patient 3) have no associated patient condition.\n",
    "\n",
    "We'll start using by passing the following prompt to 4o-mini and to o1 mini:\n",
    "```\n",
    "# Task\n",
    "Go through the following medication and condition lists and return medications for which the patient does not have an associated condition.\n",
    "\n",
    "# Input\n",
    "\n",
    "## Medications:\n",
    "[\n",
    "    (\"Patient 1\", \"Metformin\"),\n",
    "    (\"Patient 1\", \"Advil\"),\n",
    "    (\"Patient 2\", \"Humira\"),\n",
    "    (\"Patient 3\", \"Dupixent\"),\n",
    "    (\"Patient 3\", \"Creatine\"),\n",
    "]\n",
    "\n",
    "## Conditions:\n",
    "[\n",
    "    (\"Patient 1\", \"Diabetes\"),\n",
    "    (\"Patient 1\", \"Chronic Asthma\"),\n",
    "    (\"Patient 2\", \"Rheumatoid Arthritis\"),\n",
    "    (\"Patient 3\", \"Eczema\"),\n",
    "]\n",
    "```\n",
    "Screenshots of responses:\n",
    "- 4o mini: https://chatgpt.com/share/e/67d3152b-028c-8005-a131-e31a60dc07d0\n",
    "- o3-mini: https://chatgpt.com/share/e/67d314f1-9020-8005-bce1-94909e307bc8\n",
    "- o3-mini-high: https://chatgpt.com/share/e/67d31566-486c-8005-8269-ac25065cd7af\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI API\n",
    "\n",
    "In the above example, it would become tedious to continually update this prompt and paste it into ChatGPT (e.g. for patients at multiple trial sites or at various points in the trial) or to use a larger dataset. It would be easier and more scalable to use the OpenAI API to automate this process. \n",
    "\n",
    "### API Advantages\n",
    "- Automated Programmatic Workflows\n",
    "   - Enables automation of repetitive tasks, allowing you to run the same processes multiple times with different inputs efficiently (as well as batch processing!)\n",
    "- Enhanced Control Over Model Configuration\n",
    "   - Greater flexibility in choosing specific models, adjusting model parameters, and customizing system prompts\n",
    "- Production-Ready Capabilities\n",
    "   - Facilitates embedding AI capabilities directly into applications that can scale and handle large volumes of requests\n",
    "   - Developers can select and lock specific model versions and integrate with monitoring tools to track usage, performance, and error rates\n",
    "\n",
    "### API Keys\n",
    "OpenAI API keys are used to securely access the API and should be set as an environment variable when interacting with the OpenAI models.\n",
    "\n",
    "To see your individual API key(s) go to https://platform.openai.com/settings/project/api-keys. Here you can also create new API keys if you want to separately track your usage for various projects/use cases.\n",
    "\n",
    "At Genmab we use \"Project Keys\" for personal development and non-production use cases. If your use case evolves and needs a Service Key, submit a consultation request via [this link](https://teams.microsoft.com/l/entity/81fef3a6-72aa-4648-a763-de824aeafb7d/_djb2_msteams_prefix_1776558767?context=%7B%22channelId%22%3A%2219%3Af44efea3d91e4a1c9da746d204a90ff3%40thread.tacv2%22%7D&tenantId=9a88a419-24b9-401f-8557-e155db7ae966) and Farhat will get back to you. Service accounts are tied to a \"bot\" individual and should be used to provision access for production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the key for the AI Course below\n",
    " \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions API\n",
    "\n",
    "OpenAI provides simple APIs to use an LLM to generate text from a prompt, as you might using ChatGPT. The [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) is a powerful tool for building conversational agents and dynamic responses. We can use it to create dialogues, making it essential for various applications like customer service bots, virtual assistants, and more. \n",
    "\n",
    "The format of the Chat Completitions API is common and highly useful for building agents (which we'll see in a later session). The structure of multiple messages, each with a role (user, system, assistant) and content helps maintain a clear context for interactions, which is essential when building multi-turn dialogues.\n",
    "\n",
    "Each chat completitions request must specify a minimum of two parameters:\n",
    "1. `model`: the model ID to use\n",
    "2. `messages`: a list of dictionaries where each dictionary represents a message in the conversation. Each message must have:\n",
    "   - `role`: such as [`system`](https://platform.openai.com/docs/guides/text-generation#system-messages), [`user`](https://platform.openai.com/docs/guides/text-generation#user-messages), or [`assistant`](https://platform.openai.com/docs/guides/text-generation#assistant-messages)\n",
    "   - `content`: which is the main body of the message that conveys information or questions\n",
    "\n",
    "\n",
    "To get a better feel for the Chat Completions API lets try to recreate a chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "# See possible models here: https://platform.openai.com/docs/models#model-endpoint-compatibility\n",
    "client = openai.OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "# Structure of response from model described here: https://platform.openai.com/docs/api-reference/chat/object\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of the latest estimates in 2023, the population of Paris is approximately 2.1 million people within the city limits. However, if you consider the larger metropolitan area, the population is around 11 million. Please note that population figures can vary based on the source and the year.\n"
     ]
    }
   ],
   "source": [
    "# Lets add this message to our dialogue and ask a follow up question\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"And what is the population of that city?\"})\n",
    "response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we entered content that was just a string of text, but we can also pass the model content that is not text with a compatible multimodal model, like 4o-mini. Let's pass the model [this image of the Eiffel Tower](https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg/250px-Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the item in the image is located in Paris, France. It is the Eiffel Tower.\n"
     ]
    }
   ],
   "source": [
    "# Content can also be passed as a list of content parts, each with a defined type.\n",
    "new_message = {\"role\": \"user\",\n",
    "               \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Is the item in this image within that city?\"},\n",
    "                {\"type\": \"image_url\",  \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg/500px-Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg\"}}\n",
    "                ]\n",
    "                }\n",
    "\n",
    "messages.append(new_message)\n",
    "response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [the chat completion docs](https://platform.openai.com/docs/api-reference/chat/create) to learn more about other parameters that can be specified in chats, some particularly useful ones are:\n",
    "- `temperature`: controls the randomness of the modelâ€™s output. Higher values make responses more creative and diverse, while lower values make them more focused and deterministic. Default: 1.\n",
    "- `max_completion_tokens`: sets the upper limit on the number of tokens the model can generate for a response, including visible text and internal reasoning. This helps manage response length and token costs. Default: None.\n",
    "- `response_format`: specifies the output format (e.g. {\"type\": \"json_object\"} ensures the output is valid JSON). Default: None.\n",
    "- `n`: how many responses the model generates for each input. Default: 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional References\n",
    "- [OpenAI Text Generation Guide](https://platform.openai.com/docs/guides/text-generation?lang=python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompting\n",
    "\n",
    "Thus far we have directly asked the model to do tasks, without any training or examples- this is often called *\"zero-shot prompting\"*. The generalization capabilities of LLMs allow zero-shot prompting to work successfully for a large and diverse number of tasks, however in some more complex cases the model can fall short. *\"Few-shot prompting\"* is a technique where you provide the examples in prompt to steer the model to better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our medication and condition dataset and try asking 4o-mini to solve this problem for us with zero-shot prompting through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To identify the medications for which the patient does not have an associated condition, we can compare the lists of medications and conditions for each patient.\n",
      "\n",
      "### Medications:\n",
      "- Patient 1: Metformin, Advil\n",
      "- Patient 2: Humira\n",
      "- Patient 3: Dupixent, Creatine\n",
      "\n",
      "### Conditions:\n",
      "- Patient 1: Diabetes, Chronic Asthma\n",
      "- Patient 2: Rheumatoid Arthritis\n",
      "- Patient 3: Eczema\n",
      "\n",
      "### Analysis:\n",
      "1. **Patient 1**:\n",
      "   - Medications: Metformin, Advil\n",
      "   - Conditions: Diabetes, Chronic Asthma\n",
      "   - Both medications are associated with conditions, so no medications are listed here.\n",
      "\n",
      "2. **Patient 2**:\n",
      "   - Medications: Humira\n",
      "   - Conditions: Rheumatoid Arthritis\n",
      "   - Humira is associated with a condition, so no medications are listed here.\n",
      "\n",
      "3. **Patient 3**:\n",
      "   - Medications: Dupixent, Creatine\n",
      "   - Conditions: Eczema\n",
      "   - Dupixent is associated with Eczema, but Creatine has no associated condition.\n",
      "\n",
      "### Output:\n",
      "The only medication without an associated condition is:\n",
      "- Patient 3: Creatine\n",
      "\n",
      "Thus, the final output is:\n",
      "```python\n",
      "[('Patient 3', 'Creatine')]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "cm_data = [\n",
    "    (\"Patient 1\", \"Metformin\"),\n",
    "    (\"Patient 1\", \"Advil\"),\n",
    "    (\"Patient 2\", \"Humira\"),\n",
    "    (\"Patient 3\", \"Dupixent\"),\n",
    "    (\"Patient 3\", \"Creatine\"),\n",
    "]\n",
    "\n",
    "mh_data = [\n",
    "    (\"Patient 1\", \"Diabetes\"),\n",
    "    (\"Patient 1\", \"Chronic Asthma\"),\n",
    "    (\"Patient 2\", \"Rheumatoid Arthritis\"),\n",
    "    (\"Patient 3\", \"Eczema\"),\n",
    "]\n",
    "content = f\"\"\"\n",
    "        Task: Go through the following medication and condition lists and return medications for which the patient does not have an associated condition.\n",
    "        \n",
    "        ## Input\n",
    "\n",
    "        Medications:\n",
    "        {cm_data}\n",
    "\n",
    "        Conditions:\n",
    "        {mh_data}\n",
    "\n",
    "        ## Output:\n",
    "        \"\"\"\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        temperature=0,\n",
    "    )\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this answer is pretty different than what we got with 4o-mini through ChatGPT earlier. That is probably due to the fact that ChatGPT has its own system prompt (see https://x.com/krishnanrohit/status/1755122786014724125) and may be updated more frequently than the API model.\n",
    "\n",
    "Regardless, the answer is excessively wordy and only identifies one of the two correct answers. Let's try to do *\"one-shot prompting\"* and give a single example to the model to see if it improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Patient 1', 'Advil'), ('Patient 3', 'Creatine')]\n"
     ]
    }
   ],
   "source": [
    "content = f\"\"\"\n",
    "        Task: Go through the following medication and condition lists and return medications for which the patient does not have an associated condition.\n",
    "\n",
    "        ## Example:\n",
    "        Medications:\n",
    "        [(\"Patient 1\", \"Atorvastatin\"), (\"Patient 2\", \"Methotrexate\"), (\"Patient 2\", \"Simvastatin\")]\n",
    "\n",
    "        Conditions:\n",
    "        [(\"Patient 1\", \"Hyperlipidemia\"), (\"Patient 2\", \"Psoriasis\")]\n",
    "\n",
    "        Expected Output:\n",
    "        [(\"Patient 2\", \"Simvastatin\")]\n",
    "\n",
    "        ## Input\n",
    "\n",
    "        Medications:\n",
    "        {cm_data}\n",
    "\n",
    "        Conditions:\n",
    "        {mh_data}\n",
    "\n",
    "        ## Output:\n",
    "        \"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        temperature=0,\n",
    "    )\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! It is generally best practice to include at least one example in your prompt for any complex task. Some best practices for adding examples to your prompt are:\n",
    "- Use a clear and specific prompt format.\n",
    "- Use high quality and diverse examples (e.g. if you are asking for classification use positive and negative examples).\n",
    "- Iterate! See where zero-shot fails, then add examples iteratively based on failure modes.\n",
    "- There is some [research](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) to suggest using at least 2 examples, but that in-context learning may plateau after that.\n",
    "- Use tools to help you create and test prompts (e.g. Anthropic Workbench or Bedrock Prompt Management) or ask ChatGPT for help refining your prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond Few-Shot\n",
    "\n",
    "#### Files & Knowledge Bases\n",
    "Sometimes even quality examples can't get you all the way, and you might need to share additional knowledge with the model. In ChatGPT this is done through [File Uploads](https://help.openai.com/en/articles/8555545-file-uploads-faq) which allows you to upload up to 20 files (maximum size of 512 MB each) to be used for synthesis, transformation, or extraction.\n",
    "\n",
    "In the OpenAI API you can use the [Files API](https://platform.openai.com/docs/api-reference/files) to upload and use files in [Assistants](https://platform.openai.com/docs/api-reference/assistants), [Fine-tuning](https://platform.openai.com/docs/api-reference/fine-tuning), or Batch API. Support for files in Chat Completitions was recently added but only for [PDF files](https://platform.openai.com/docs/guides/pdf-files?api-mode=chat).\n",
    "\n",
    "Let's look at medical coding to the MedDRA dictionary and consider some other ways we could give the model extra knowledge outside of few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llt_code</th>\n",
       "      <th>llt_name</th>\n",
       "      <th>pt_code</th>\n",
       "      <th>llt_currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000001</td>\n",
       "      <td>Ventilation pneumonitis</td>\n",
       "      <td>10081988</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000002</td>\n",
       "      <td>11-beta-hydroxylase deficiency</td>\n",
       "      <td>10000002</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000003</td>\n",
       "      <td>11-oxysteroid activity incr</td>\n",
       "      <td>10033315</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000004</td>\n",
       "      <td>11-oxysteroid activity increased</td>\n",
       "      <td>10033315</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000005</td>\n",
       "      <td>17 ketosteroids urine</td>\n",
       "      <td>10000005</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   llt_code                          llt_name   pt_code llt_currency\n",
       "0  10000001           Ventilation pneumonitis  10081988            N\n",
       "1  10000002    11-beta-hydroxylase deficiency  10000002            Y\n",
       "2  10000003       11-oxysteroid activity incr  10033315            N\n",
       "3  10000004  11-oxysteroid activity increased  10033315            Y\n",
       "4  10000005             17 ketosteroids urine  10000005            Y"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a small version (~30K/80K total codes) of the most recent MedDRA dictionary \n",
    "df = pd.read_csv(\"meddra_27_1_llt_small.txt\", sep=\" \", header=None, dtype=str)\n",
    "df.columns = [\"llt_code\", \"llt_name\", \"pt_code\", \"llt_currency\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llt_code</th>\n",
       "      <th>llt_name</th>\n",
       "      <th>pt_code</th>\n",
       "      <th>llt_currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7918</th>\n",
       "      <td>10008229</td>\n",
       "      <td>Cervical cancer</td>\n",
       "      <td>10008342</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>10008231</td>\n",
       "      <td>Cervical cancer recurrent</td>\n",
       "      <td>10008344</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7920</th>\n",
       "      <td>10008232</td>\n",
       "      <td>Cervical cancer stage 0</td>\n",
       "      <td>10061809</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7921</th>\n",
       "      <td>10008233</td>\n",
       "      <td>Cervical cancer stage I</td>\n",
       "      <td>10008345</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>10008234</td>\n",
       "      <td>Cervical cancer stage II</td>\n",
       "      <td>10008346</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7923</th>\n",
       "      <td>10008235</td>\n",
       "      <td>Cervical cancer stage III</td>\n",
       "      <td>10008347</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>10008236</td>\n",
       "      <td>Cervical cancer stage IV</td>\n",
       "      <td>10008348</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      llt_code                   llt_name   pt_code llt_currency\n",
       "7918  10008229            Cervical cancer  10008342            Y\n",
       "7919  10008231  Cervical cancer recurrent  10008344            Y\n",
       "7920  10008232    Cervical cancer stage 0  10061809            Y\n",
       "7921  10008233    Cervical cancer stage I  10008345            Y\n",
       "7922  10008234   Cervical cancer stage II  10008346            Y\n",
       "7923  10008235  Cervical cancer stage III  10008347            Y\n",
       "7924  10008236   Cervical cancer stage IV  10008348            Y"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try to get GPT to help us find the appropriate code for recurrent cervical cancer\n",
    "condition = \"recurrent cervical cancer\"\n",
    "df[df[\"llt_name\"].str.contains(\"cervical cancer\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The appropriate MedDRA LLT code for \"recurrent cervical cancer\" is \"10012600\".\n"
     ]
    }
   ],
   "source": [
    "system_message = (\"You are a skilled medical coder translating natural language medical history conditions and adverse events \"\n",
    "                  \"to codes from the MedDRA dictionary.\")\n",
    "input_message = f\"\"\"Determine the appropriate MedDRA LLT code for the following medical condition.\n",
    "                # Example:\n",
    "                ## Input: \"ventilation pneumonitis\"\n",
    "                ## Output: \"10000001\"\n",
    "                \n",
    "                # Input:\n",
    "                {condition}\n",
    "                \"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": input_message}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llt_code</th>\n",
       "      <th>llt_name</th>\n",
       "      <th>pt_code</th>\n",
       "      <th>llt_currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>10012600</td>\n",
       "      <td>Diabetes insipidus nephrogenic</td>\n",
       "      <td>10029147</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       llt_code                        llt_name   pt_code llt_currency\n",
       "12126  10012600  Diabetes insipidus nephrogenic  10029147            Y"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check to see if that was correct\n",
    "output_code = \"10012600\"\n",
    "df[df[\"llt_code\"] == output_code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is unable to code correctly, let's instead give the model the dictionary file as a source of knowledge and ask again. We could do this in ChatGPT with a file upload, or with the OpenAI API we can use the new [Responses API](https://platform.openai.com/docs/guides/responses-vs-chat-completions) with file search capabilities. \n",
    "\n",
    "The Responses API offers a new way to interact with OpenAI models, similar to Chat Completitions, but is built with agents in mind. This means it gives users access to tools that are common in agentic tasks like web search, file search, and computer use. The Responses API replaces OpenAI's previous agentic framework, called [Assistants](https://platform.openai.com/docs/assistants/overview) which will be deprecated in 2026. If you are new to the OpenAI platform, it is probably wise to start using Responses inplace of Chat Completitions as your API for interacting with OpenAI models. \n",
    "\n",
    "For our MedDRA problem let's use the file search tool in the Responses API, which enables models to retrieve information from a knowledge base of uploaded files. We begin by uploading a file to the Files API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic method to use for uploading files (from URL for file path) to Files API\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def create_file(client, file_path):\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(\n",
    "            file=file_tuple,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(\n",
    "                file=file_content,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "    print(result.id)\n",
    "    return result.id\n",
    "\n",
    "# Replace with your own file path or URL\n",
    "# file_id = create_file(client, \"meddra_27_1_llt_small.txt\")\n",
    "\n",
    "# To keep costs down the above line has been run once and we will all use the below file_id generated\n",
    "file_id = \"file-Bkod9mSgqMezv6E2eJNaeA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The file_search tool uses the Vector Store object for storing and searching file content. Adding a file to a vector store automatically parses, chunks, embeds and stores the file in a vector database that's capable of both keyword and semantic search (more on this to come in the RAG section later). Each vector store can hold up to 10,000 files. \n",
    "\n",
    "For our simple example we will create a vector store, upload and add our file to the vector store, then give the vector store to the assistant. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreFile(id='file-Bkod9mSgqMezv6E2eJNaeA', created_at=1742338125, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_67d9f5c8516c8191a000cd88e1284edb', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store\n",
    "# vector_store = client.vector_stores.create(name=\"knowledge_base\")\n",
    "\n",
    "# To keep costs down the above line has been run once and we will all use the vector store generated\n",
    "vector_store_id = \"vs_67d9f5c8516c8191a000cd88e1284edb\"\n",
    " \n",
    "# Add a file to a vector store\n",
    "client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store_id,\n",
    "    file_id=file_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_67d9f8666d0081918ec9cea7013a70e804247f0def6dbe29', created_at=1742338150.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFileSearchToolCall(id='fs_67d9f8672c6081919194c0ca663f25b504247f0def6dbe29', queries=['MedDRA LLT code for recurrent cervical cancer', 'recurrent cervical cancer MedDRA code', 'MedDRA LLT recurrent cervical cancer'], status='completed', type='file_search_call', results=None), ResponseOutputMessage(id='msg_67d9f86b385881919dfd5383ceb4de0904247f0def6dbe29', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='file-9h5q5GgJbi4L6YiFeAurXE', index=86, type='file_citation', filename='meddra_27_1_llt_small.txt')], text='The MedDRA LLT (Lower Level Term) code for \"recurrent cervical cancer\" is **10008344**.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_67d9f5c8516c8191a000cd88e1284edb'], filters=None, max_num_results=20, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=17596, output_tokens=85, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=17681, input_tokens_details={'cached_tokens': 0}), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "content = f\"What is the MedDRA LLT code for the following medical condition: {condition}\"\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=content,\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store_id]\n",
    "    }])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the file search tool is called by the model, you will receive a response with multiple outputs:\n",
    "1. A file_search_call output item, which contains the id of the file search call.\n",
    "2. A message output item, which contains the response from the model, along with the file citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MedDRA LLT (Lower Level Term) code for \"recurrent cervical cancer\" is **10008344**.\n"
     ]
    }
   ],
   "source": [
    "# The first text response from the model\n",
    "print(response.output[1].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llt_code</th>\n",
       "      <th>llt_name</th>\n",
       "      <th>pt_code</th>\n",
       "      <th>llt_currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>10008231</td>\n",
       "      <td>Cervical cancer recurrent</td>\n",
       "      <td>10008344</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      llt_code                   llt_name   pt_code llt_currency\n",
       "7919  10008231  Cervical cancer recurrent  10008344            Y"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check to see if that was correct\n",
    "output_code = \"10008231\"\n",
    "df[df[\"llt_code\"] == output_code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using file search in Responses API is a great precursor to understanding Retrieval-Augmented Generation (RAG) systems, a topic which will be covered in a later module. By integrating file search, you are storing information from files in a vector store which can then be used to dynamically retrieve relevant information and feed that context to the model. This approach allows the LLM to augment its responses with factual data stored externally.\n",
    "\n",
    "Like with other RAG systems, you can fine-tune some parameters to optimize search precision and relevance. For example, you can customize the number of results retrieved from the vector store, and add metadata filtering to enhance the quality of retrieved content. To explore these concepts further check out the [OpenAI File Search Guide](https://platform.openai.com/docs/guides/tools-file-search#retrieval-customization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning\n",
    "Sometimes just passing a file or knowledge base to the model may not be sufficient, and in *some* of these cases fine-tuning can help. Fine-tuning is the process of adapting an existing model to perform better for a specific task by training it on a task-specific dataset. This process requires a large, well-structured dataset and usually a **significant amount of time, effort, and cost**. It is always recommended to first explore prompt engineering, integrating knowledge bases, and/or leveraging function calling to achieve desired results before considering fine-tuning.\n",
    "\n",
    "Some common use cases where fine-tuning can improve results: \n",
    "- Handling many edge cases in specific ways\n",
    "- Setting the style, tone, format, or other qualatative aspects in ways that's hard to articulate in a prompt\n",
    "- Improving reliability at producing a desired output\n",
    "\n",
    "For some long-term projects using LLMs, fine-tuning can reduce cost and/or latency associated with repeatedly sending lengthy prompts. In these cases the upfront cost of fine-tuning and hosting a fine-tuned model can pay off over time. An increasingly common trend is to replace usage of larger LLMs (like gpt-4o) with a fine-tuned smaller model (like gpt-4o-mini) to achieve cost savings and maintain or improve performance.\n",
    "\n",
    "Returning to our medical coding example we can consider how companies often accumulate large datasets of historical coding decisions. These datasets often contain the most challenging cases for medical coding (e.g. where conditions are represented differently than in the dictionary and may contain abbreviations and/or specialized terminology). In this context, it might be useful to fine-tune a smaller model with the historical coding dataset to create a company-specific medical coding model that makes decisions similar to past coding decisions.\n",
    "\n",
    "Because most of us are using LLMs for proof-of-concept and shorter-term projects, we won't cover the specifics of fine-tuning with the OpenAI API here, but refer to the [OpenAI fine-tuning docs](https://platform.openai.com/docs/guides/fine-tuning) to learn more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
