{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Advanced Prompting Techniques\n",
    "\n",
    "This module introduces you to additional prompting techniques that can improve the performance of your LLM application. This includes prompt engineering techniques as well as higher level code frameworks that can help build a system that is consistent and can be integrated into a larger codebase or application. We will end by introducing a modern LLM framework (Dspy) that abstracts away a lot of the baseline API configuration, and has higher level functionality like the ability to optimize your prompts for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import pydantic\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the key for the AI Course below\n",
    " \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm API connection works\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best programming language for LLMs?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out all of the available models\n",
    "# Chat completions API compatibility: https://platform.openai.com/docs/models#model-endpoint-compatibility\n",
    "models_list = client.models.list().data\n",
    "for model in models_list:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting 'Engineering' Techniques\n",
    "\n",
    "![ontology](PA-ontology.png)\n",
    "\n",
    "There are a lot of techniques that you can leverage when writing out prompts to elicit expected results. This paper has a pretty comprehensive review of techniques and when to use them: [link to paper](https://arxiv.org/pdf/2406.06608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prompt_call(prompt, model='gpt-4o-mini'):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a basic question without any examples\n",
    "prompt = \"\"\"Analyze the sentiment of this patient feedback:\n",
    "‘Oh, great! Another drug that claims to work wonders but just emptied my wallet.'\"\"\"\n",
    "\n",
    "response = single_prompt_call(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with few-shot prompting\n",
    "prompt = \"\"\"Here are examples of sentiment analysis for patient feedback:\n",
    "\t1.\tFeedback: Drug X worked wonders for my migraines. Im so grateful!\n",
    "Sentiment: Positive\n",
    "\t2.\tFeedback: I had severe side effects with Drug Y and had to stop taking it.\n",
    "Sentiment: Negative\n",
    "\t3.\tFeedback: Drug Z was okay—it helped, but not as much as I hoped.\n",
    "Sentiment: Mixed\n",
    "\n",
    "Now, analyze the sentiment of this patient feedback:\n",
    "Oh, great! Another drug that claims to work wonders but just emptied my wallet.\"\"\"\n",
    "\n",
    "response = single_prompt_call(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Chaining\n",
    "\n",
    "This is the technique primarily used when building out chatbots to preserve the history of previous messages as relevant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_with_messages(messages, model='gpt-4o-mini'):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original message\n",
    "messages=[\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the python programming language?\"\n",
    "        }\n",
    "        ]\n",
    "\n",
    "response = call_with_messages(messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can append this answer to our messages list\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response.choices[0].message.content\n",
    "})\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add a follow-up question that refers to what was originally asked\n",
    "prompt = \"Tell me more\"\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt\n",
    "})\n",
    "\n",
    "response = call_with_messages(messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought\n",
    "\n",
    "![CoT](PA-CoT.png)\n",
    "\n",
    "[Arxiv Paper Link](https://arxiv.org/pdf/2201.11903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_extract_translation = \"\"\"¡Preparar café Cold Brew es un proceso sencillo y refrescante!\n",
    "Todo lo que necesitas son granos de café molido grueso y agua fría.\n",
    "Comienza añadiendo el café molido a un recipiente o jarra grande.\n",
    "Luego, vierte agua fría, asegurándote de que todos los granos de café\n",
    "estén completamente sumergidos.\n",
    "Remueve la mezcla suavemente para garantizar una saturación uniforme.\n",
    "Cubre el recipiente y déjalo en remojo en el refrigerador durante al\n",
    "menos 12 a 24 horas, dependiendo de la fuerza deseada.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prompt = f\"\"\"\n",
    "Question: Give me a numbered list of all coffee-related words in English from the text below:\n",
    "\n",
    "Text: {text_to_extract_translation}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "baseline_response = single_prompt_call(baseline_prompt)\n",
    "print(baseline_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it give the list in english? This didn't follow the instructions well because it needs to do a few steps of reasoning before outputting the response. \n",
    "\n",
    "Lets try prompting it to break it into separate steps with an example (Chain of thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Chain of Thought\n",
    "\n",
    "CoT_prompt = f\"\"\"\n",
    "Question: Give me a numbered list of all tennis-related words in English from the text below:\n",
    "\n",
    "Text: Andre Agassi, una leyenda del tenis con ocho títulos de Grand Slam, fue celebrado por su poderoso juego de fondo y sus implacables devoluciones. \n",
    "Más allá de la corte, defendió la educación y fundó la Fundación Andre Agassi para jóvenes desfavorecidos. \n",
    "Sus memorias, Open, revelan su viaje de resiliencia, pasión y reinvención, inspirando a innumerables personas en todo el mundo.\n",
    "\n",
    "Answer: The spanish words that are related to tennis are: tenis, juego de fondo, devoluciones. These words in english are: tennis, baseline, returns\n",
    "\n",
    "\n",
    "Question: Give me a numbered list of all coffee-related words in English from the text below:\n",
    "\n",
    "Text: {text_to_extract_translation}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "CoT_response = single_prompt_call(CoT_prompt)\n",
    "print(CoT_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come up with another text to test whether the chain of thought prompt gives an accurate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree of Thought\n",
    "![ToT](PA-ToT.png)\n",
    "\n",
    "[Tree of thought Arxiv Paper](https://arxiv.org/pdf/2305.10601)\n",
    "\n",
    "- The premise here is to generate a tree of expanding possible intermediate steps to fulfill the overall goal\n",
    "- Bread-first-search (BFS) or Depth-first-search (DFS) can be used to traverse the tree of steps until a valid outcome is identified\n",
    "\n",
    "\n",
    "We won't be coding this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task used in Tree of Thought was the mathematical reasoning challenge: Game of 24\n",
    "- Given an input of 4 integers\n",
    "- Use the 4 integers with any combination of basic arithmetic operations (+-*/) to obtain 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Game of 24](PA-ToT24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Models\n",
    "\n",
    "Multimodal models combine the ability of transformers to understand data across multiple modalities. The most common forms combine the language modality with images and video. This will be a demonstration of how to use images as inputs to multimodal models.\n",
    "\n",
    "Documentation on this API: https://platform.openai.com/docs/guides/vision\n",
    "\n",
    "Lets test this with a chart of EPCORE results:\n",
    "![DOR_Chart](epcore_DOR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image file path\n",
    "image_path = \"epcore_DOR.png\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What is in this image?\",\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\":  f\"data:image/png;base64,{base64_image}\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets try asking it questions and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_image_questions(filepath, prompt, model=\"gpt-4o-mini\"):\n",
    "    base64_image = encode_image(filepath)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt,\n",
    "            },\n",
    "            {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\":  f\"data:image/png;base64,{base64_image}\"\n",
    "            },\n",
    "            },\n",
    "        ],\n",
    "        }\n",
    "    ],\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the duration of response for 15th month mark?\"\n",
    "response = ask_image_questions(image_path, prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How many subjects are at risk at the 15th month mark?\"\n",
    "response = ask_image_questions(image_path, prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try using another model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try another example with another chart type\n",
    "![ORR_Chart](epcore_ORR.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What was the ORR for patients without Prior CAR-T experience?\"\n",
    "response = ask_image_questions('epcore_ORR.png', prompt, model=\"gpt-4o-mini\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "There are many instances when you want the LLM to provide responses in a specific format. This is most important when you want to build out more sophisticated systems, with subsequent calls, or if you simply need the output of the LLM to fit into a specific data structure. Structured outputs allows you to specify this when calling the API.\n",
    "\n",
    "This is where Pydantic comes into play. It is a way to programmatically describe the data structure you want as an output.\n",
    "\n",
    "Lets try to make the LLM extract information from a clinical trial protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_description = \"\"\"The drug that will be investigated in the study is GEN1053. GEN1053 is an antibody designed to (re)activate and increase antitumor immunity.\n",
    "\n",
    "Since this is the first study of GEN1053 in humans, the main purpose is to evaluate safety. Besides safety, the study will determine the recommended GEN1053 dose to be tested in a larger group of participants and assess preliminary clinical activity of GEN1053.\n",
    "\n",
    "GEN1053 will be studied in a broad group of cancer patients, having different kinds of solid tumors. All participants will get GEN1053. The study consists of two parts: Part 1 tests increasing doses of GEN1053 (\"escalation\"), followed by Part 2 which tests the recommended phase 2 dose GEN1053 dose from Part 1 (\"expansion\").\n",
    "\n",
    "The trial is a First in Human open-label, multicenter, multinational safety trial in participants with non-central nervous system (non-CNS) metastatic or advanced malignant solid tumors for whom there is no available standard therapy likely to confer clinical benefit, evaluating the safety, tolerability, preliminary antitumor activity, pharmacokinetics (PK), pharmacodynamics (PD), and immunogenicity of GEN1053.\n",
    "\n",
    "The trial will be conducted as follows:\n",
    "\n",
    "The Dose Escalation part (Part 1) will explore the safety of escalating doses of GEN1053 as monotherapy (phase 1)\n",
    "The Expansion part (Part 2) is planned to provide additional safety and initial antitumor activity information of the Recommended Phase 2 dose (RP2D) for GEN1053 monotherapy in selected tumor indications, as well as more detailed data related to the mode of action (MoA).\"\"\"\n",
    "\n",
    "baseline_study_prompt = f\"\"\"Extract the drug name and how many parts there are of the trial from the following clinical trial description:\n",
    "{trial_description}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_study_response = single_prompt_call(baseline_study_prompt)\n",
    "print(baseline_study_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if you wanted to parse these two responses out into an input for a database? \n",
    "\n",
    "Regex statement to extract the two answers out? (if you dont know how to create a regex pattern, try ChatGPT to help you)\n",
    "\n",
    "What if there are extraneous characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_study_prompt = f\"\"\"Extract the drug name and how many parts there are of the trial from the following clinical trial description:\n",
    "{trial_description}\n",
    "\n",
    "Format your response like the following:\n",
    "drug_name: drug name from clinical trial description\n",
    "parts: count of parts of the study\n",
    "\"\"\"\n",
    "\n",
    "formatted_study_response = single_prompt_call(formatted_study_prompt)\n",
    "print(formatted_study_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can parse it out easier, but what if you need to ensure the parts is an integer because you need to programmatically process the output differently depending on the answer?\n",
    "\n",
    "How do you make sure the output conforms to your expectations so no errors occur when running your script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create this \"pattern\" by creating a class inheriting from the pydantic BaseModel class\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class StudyOutput(BaseModel):\n",
    "    drug: str\n",
    "    parts: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_prompt = f\"\"\"Extract the drug name and how many parts there are of the trial from the following clinical trial description:\n",
    "{trial_description}\"\"\"\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": study_prompt},\n",
    "  ],\n",
    "  response_format=StudyOutput,\n",
    ")\n",
    "\n",
    "response_object = response.choices[0].message.parsed\n",
    "response_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a pydantic class to extract a list of trial part descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to DSPY\n",
    "\n",
    "A framework for programming with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "oa_model = dspy.OpenAI(model='gpt-4o-mini', max_tokens=250)\n",
    "dspy.settings.configure(lm=oa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signatures are a way of functionalizing prompts. Create a signature with the class argstring being the prompt instructions. Inputs and outputs are defined with `dspy.InputField` and `dspy.OutputField` objects\n",
    "\n",
    "Similar to how you can define the expected output format with the openai chat completions api, you can define the structure of the outputs with DSPY signatures as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTParse(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Extract the drug name from the following clinical trial description.\n",
    "    \"\"\"\n",
    "    ct_description: str = dspy.InputField(desc=\"Clinical Trial description\")\n",
    "    drug: str = dspy.OutputField(desc=\"The drug name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_parser = dspy.ChainOfThought(CTParse)\n",
    "\n",
    "parse_result = ct_parser(ct_description=trial_description)\n",
    "parse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now update the dspy signature to output the # of parts of the trial as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Prompt Optimization with DSPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dspy.datasets import HotPotQA\n",
    "\n",
    "# dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# trainset, devset = dataset.train, dataset.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the dataset from file\n",
    "import pickle\n",
    "\n",
    "with open(\"hotpotqa_train.pkl\", \"rb\") as f:\n",
    "    trainset = pickle.load(f)\n",
    "\n",
    "with open(\"hotpotqa_dev.pkl\", \"rb\") as f:\n",
    "    devset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTSignature(dspy.Signature):\n",
    "    \"\"\"Answer the question and give the reasoning for the same.\"\"\"\n",
    "\n",
    "    question = dspy.InputField(desc=\"question about something\")\n",
    "    reasoning = dspy.OutputField(desc=\"reasoning for the answer\")\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "class CoTPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.signature = CoTSignature\n",
    "        self.predictor = dspy.ChainOfThought(self.signature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        result = self.predictor(question=question)\n",
    "        return dspy.Prediction(\n",
    "            answer=result.answer,\n",
    "            reasoning=result.reasoning,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = CoTPipeline()\n",
    "test_out = test_pipe('give me an answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    return answer_EM\n",
    "\n",
    "NUM_THREADS = 5\n",
    "evaluate = Evaluate(devset=devset, metric=validate_context_and_answer, num_threads=NUM_THREADS, display_progress=True, display_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_baseline = CoTPipeline()\n",
    "\n",
    "devset_with_input = [dspy.Example({\"question\": r[\"question\"], \"answer\": r[\"answer\"]}).with_inputs(\"question\") for r in devset]\n",
    "evaluate(cot_baseline, devset=devset_with_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import COPRO\n",
    "\n",
    "teleprompter = COPRO(\n",
    "    metric=validate_context_and_answer,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(num_threads=64, display_progress=True, display_table=0) # Used in Evaluate class in the optimization process\n",
    "\n",
    "compiled_prompt_opt = teleprompter.compile(cot_baseline, trainset=devset_with_input, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the updated signature after optimization\n",
    "\n",
    "compiled_prompt_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the necessary changes to the signature to match the optimized prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Implement Tree-of-Thought using structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
