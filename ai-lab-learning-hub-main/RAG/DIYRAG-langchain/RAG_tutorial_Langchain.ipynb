{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Retrieval-Augmented Generation (RAG) for Biotechnology: A Hands-On Starter Guide\n",
    "\n",
    "\"\"\"\n",
    "Welcome to our RAG tutorial‚Äîyour gateway to transforming how Genmab‚Äôs researchers access data.\n",
    "\n",
    "In this notebook, we build a foundational RAG system that:\n",
    "‚Ä¢ **Retrieves** contextual information from scientific literature (e.g., PubMed abstracts)\n",
    "‚Ä¢ **Augments** GPT-4‚Äôs generative capabilities with domain-specific data\n",
    "‚Ä¢ **Generation** Outputs insightful, accurate, and actionable responses tailored for biotechnology applications\n",
    "\n",
    "This DIY RAG setup demystifies the components behind:\n",
    "1. Document splitting and embedding creation\n",
    "2. Vector storage and retrieval\n",
    "3. LLM-driven answer generation\n",
    "\n",
    "Consider this the \"training wheels\" stage‚Äîby mastering these basics.\n",
    "\n",
    "Let‚Äôs dive in and see how we can turn our wealth of scientific data into strategic insights!\n",
    "\"\"\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 3px solid white; width: 100%;\">\n",
    "<hr style=\"border: 3px solid white; width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## **Motivation & Background** <a id=\"motivatio-&-background\"></a>\n",
    "\n",
    "\n",
    "Large language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks. These limitations often manifest as **hallucinations** ‚Äîanswers that sound plausible but are factually incorrect‚Äîwhen handling queries beyond the model‚Äôs training data or requiring current information.\n",
    "\n",
    "To address these challenges, **Retrieval-Augmented Generation (RAG)** incorporates external knowledge sources. By retrieving relevant document chunks through semantic similarity, RAG mitigates factual inaccuracies and keeps responses up to date. This integration ensures LLMs have the context they need to remain both accurate and current, fostering widespread adoption of RAG in real-world applications.\n",
    "\n",
    "### Why Use RAG?\n",
    "\n",
    "1. **Access to Fresh Information**  \n",
    "   LLMs rely on static training corpora, risking outdated responses. RAG taps into external, dynamic databases‚Äîproviding the latest facts and cutting-edge data.  \n",
    "\n",
    "2. **Factual Grounding**  \n",
    "   LLMs excel at generating fluent text but can falter on factual correctness. By feeding retrieved text directly into the prompt, RAG reduces hallucinations and ensures evidence-based answers.\n",
    "\n",
    "3. **Scalability & Efficiency**  \n",
    "   Even with long context windows, LLMs have token limits. RAG‚Äôs retrieval stage selectively brings in only the most relevant chunks, which saves costs and tokens while boosting relevance.\n",
    "\n",
    "4. **Semantic Search & Re-Rankers**  \n",
    "   Modern RAG systems often leverage vector databases (for semantic search), possible keyword-based fallback, and re-rankers that ensure the top results are truly on-topic.\n",
    "\n",
    "5. **Quality & Reliability**  \n",
    "   By anchoring generated content to curated knowledge, RAG helps maintain consistency and accuracy. This is essential for domains like biotechnology, finance, or medicine, where mistakes can have serious consequences.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 3px solid white; width: 100%;\">\n",
    "<hr style=\"border: 3px solid white; width: 100%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation, Background & Key Components\n",
    "\n",
    "Large language models (LLMs) have transformed our approach to text and information, yet they sometimes \"hallucinate\" when handling domain-specific or knowledge-intensive queries. This occurs because LLMs rely on static training data that doesn‚Äôt capture the latest, domain-specific nuances.\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** fixes this gap by integrating up-to-date external information. It retrieves relevant segments from a curated knowledge base and merges this context with the LLM's generative power to produce answers that are both fluent and factually grounded‚Äîessential for high-stakes fields like biotechnology.\n",
    "\n",
    "### Key Components of a RAG System\n",
    "\n",
    "1. **Document Preprocessing & Chunking:**  \n",
    "   Break long documents into manageable, semantically coherent chunks.\n",
    "\n",
    "2. **Embedding Generation:**  \n",
    "   Convert chunks into numerical vectors using models (e.g., OpenAI‚Äôs embeddings).\n",
    "\n",
    "3. **Vector Database & Semantic Search:**  \n",
    "   Store embeddings to enable similarity-based retrieval of relevant text.\n",
    "\n",
    "4. **Prompt Augmentation:**  \n",
    "   Combine retrieved context with the original query to ‚Äúground‚Äù the LLM‚Äôs response.\n",
    "\n",
    "5. **Generative Answer Synthesis:**  \n",
    "   The LLM processes the augmented prompt to generate responses that are precise and domain-tailored.\n",
    "\n",
    "### Why Use RAG?\n",
    "\n",
    "- **Access to Fresh Information:**  \n",
    "  RAG taps into dynamic external databases, ensuring responses reflect current data rather than outdated training corpora.\n",
    "\n",
    "- **Factual Grounding:**  \n",
    "  By integrating real-world information directly into the prompt, RAG minimizes hallucinations and supports evidence-based answers.\n",
    "\n",
    "- **Efficient Context Management:**  \n",
    "  It selectively pulls in only the most relevant pieces‚Äîkeeping token usage economical and responses focused.\n",
    "\n",
    "- **Enhanced Retrieval Performance:**  \n",
    "  Modern RAG systems utilize vector searches, keyword fallback, and re-ranking to ensure high-quality context is used for answer generation.\n",
    "\n",
    "- **Quality & Reliability for Critical Domains:**  \n",
    "  Anchoring outputs in curated external knowledge helps maintain consistency and trustworthiness‚Äîvital in fields such as biotechnology.\n",
    "\n",
    "### Supporting Diagram\n",
    "\n",
    "Below is a visual outline of the RAG evolution‚Äîfrom naive to advanced and modular architectures‚Äîwhich frames our approach within the broader continuum:\n",
    "\n",
    "This diagram reinforces our message: as we move from basic DIY RAG toward a fully managed solution, we gain scalability and ease-of-use while still retaining the accuracy and reliability essential for scientific research.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 3px solid white; width: 100%;\">\n",
    "<hr style=\"border: 3px solid white; width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RAG Architectural Diagram** <a id=\"architectural-diagram\"></a>\n",
    "\n",
    "\n",
    "![RAG Paradigm](rag_paradigm.png)\n",
    "\n",
    "*Figure 1: Overview of the Retrieval-Augmented Generation System Architecture*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 3px solid white; width: 100%;\">\n",
    "<hr style=\"border: 3px solid white; width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Practical Implementation** <a id=\"practical-implementation\"></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG) system** tailored for biotechnology applications.\n",
    "\n",
    "### Key Steps:\n",
    "- Load and process a small subset of **PubMed** data.\n",
    "- Convert data into **LangChain Document** objects.\n",
    "- Split documents into **manageable chunks** for semantic retrieval.\n",
    "- Build a **vector index** using **FAISS** with domain-specific embeddings.\n",
    "- Construct a **Retrieval-Augmented Generation** (RAG) chain powered by **ChatGPT**.\n",
    "- Run example queries and display results.\n",
    "\n",
    "**Note:** For production use, ensure secure management of **environment variables** and **API keys**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# Environment Setup: API Key and Model Name\n",
    "# ------------------------------\n",
    "# In this example, we set the OpenAI API key for ChatGPT-based generation.\n",
    "# For production, load the key securely (e.g., from environment variables or a config file).\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"_\"  # Replace \"_\" with your actual OpenAI API key if needed.\n",
    "# Ensure the API key is set; otherwise, raise a warning.\n",
    "if os.environ.get(\"OPENAI_API_KEY\") is None or os.environ.get(\"OPENAI_API_KEY\") == \"_\":\n",
    "    print(\"Warning: OPENAI_API_KEY is not set securely. Please configure it properly for production use.\")\n",
    "\n",
    "# Set the model name used for chat generation.\n",
    "# You can switch models based on your requirements.\n",
    "model_name = \"gpt-4o-mini\"  \n",
    "# Alternative model example (commented out):\n",
    "# model_name = \"o3-mini-2025-01-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing and Indexing** <a id=\"data-preprocessing-and-indexing\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Idexing Paradigm](indexing.jpeg)\n",
    "\n",
    "*Figure 2: Indexing Process*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain components for handling documents, text splitting, and chain construction.\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#\n",
    "# This section converts raw data (PubMed abstracts) into LangChain Document objects,\n",
    "# which are easier to process for embedding and retrieval.\n",
    "#\n",
    "# The load_dataset function fetches a subset of PubMed articles from the \"scientific_papers\" dataset.\n",
    "# from datasets import load_dataset \n",
    "# pubmed_data = load_dataset('scientific_papers', 'pubmed', split='train[:1%]', trust_remote_code=True)\n",
    "#\n",
    "# To save time during demonstrations, we have pre-saved the abstracts from this subset read them in below.\n",
    "with open(\"pubmed_abstracts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pubmed_data = json.load(f)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Converting the dataset to LangChain Document objects.\n",
    "# We use only the 'abstract' field for demonstration. If an item has an abstract, it is converted into a Document.\n",
    "# The Document objects include page_content (the abstract) and optional metadata.\n",
    "# ------------------------------\n",
    "documents = []\n",
    "for item in pubmed_data:\n",
    "    documents.append(Document(page_content=item))\n",
    "print(f\"Loaded {len(documents)} documents from PubMed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------------------------\n",
    "## **Splitting Documents into Chunks**\n",
    " ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Document Chunking for Effective Retrieval\n",
    "#\n",
    "# To ensure our LLM stays within token limits and focuses on the most relevant\n",
    "# portions of our documents, we first split each document into smaller chunks.\n",
    "#\n",
    "# In this implementation, we use a straightforward, character-based chunking approach:\n",
    "#   ‚Ä¢ **chunk_size:** Maximum number of characters per chunk (e.g., 1000).\n",
    "#   ‚Ä¢ **chunk_overlap:** We allow some overlap (e.g., 200 characters) between consecutive chunks\n",
    "#     to help preserve context at the boundaries.\n",
    "#\n",
    "# This \"brute force\" method is simple, fast, and effective for many applications.\n",
    "#\n",
    "# Note: More semantically informed methods‚Äîsuch as sentence-based chunking‚Äîcan further preserve\n",
    "# natural language boundaries and coherence. While those methods can offer better context preservation,\n",
    "# they require additional processing and complexity.\n",
    "#\n",
    "# Here, we use our character-based splitter as a baseline for this RAG demo.\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,   # Maximum characters per chunk; can be adjusted based on model limits.\n",
    "    chunk_overlap=200  # Overlap to maintain context between chunks.\n",
    ")\n",
    "docs_split = splitter.split_documents(documents)\n",
    "print(f\"Number of chunks after splitting: {len(docs_split)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## **Building the Embedding and Vector Store (Indexing)**\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Embedding and Vector Store Setup with FAISS\n",
    "#\n",
    "# In this step, we leverage OpenAI‚Äôs embedding model to convert our document chunks \n",
    "# into numerical vectors. We then use FAISS‚Äîa fast similarity search library‚Äîto store and index these embeddings.\n",
    "#\n",
    "# Although we use OpenAI‚Äôs general embeddings here, note that domain-specific embeddings (for biomedical text) might yield improved performance in specialized applications.\n",
    "#\n",
    "# FAISS provides a scalable solution for semantic similarity search, ensuring efficient storage and rapid retrieval of relevant text chunks.\n",
    "# We then create a retriever that fetches the top 5 most relevant chunks for any query.\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Instantiate the OpenAI embedding model.\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create a FAISS vector store from the pre-split document chunks.\n",
    "vectorstore = FAISS.from_documents(\n",
    "    docs_split,      # The document chunks produced earlier.\n",
    "    embedding_model  # Compute embeddings using OpenAI's model.\n",
    ")\n",
    "\n",
    "# Create a retriever from the FAISS vector store.\n",
    "# 'search_kwargs' (here, k=5) limits the results to the top 5 most relevant chunks.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Retrieval and Generation Paradigm](prompt-and-generation.jpeg)\n",
    "\n",
    "*Figure 2: Retrieval and Generation Paradigm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "### **Building the Retrieval-Augmented Generation (RAG) Chain with ChatOpenAI**\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we switch to the generative component that uses ChatGPT for answer generation.\n",
    "#\n",
    "# The ChatOpenAI component creates an instance of the chat-model.\n",
    "# Here, we use a model (gpt-4o-mini or another specified) with temperature=0 for focused, deterministic responses.\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm_chatgpt = ChatOpenAI(\n",
    "    model_name=model_name,                 # Model name as defined earlier\n",
    "    temperature=0,                         # Temperature 0 for precise answers in domain-specific questions\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")  # Use the API key from the environment\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## **Define a prompt template for the RAG chain.**\n",
    "### The template instructs the LLM to use the provided context and answer the question.\n",
    "### It also instructs the model to state if the context is insufficient.\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following context to answer the question. \n",
    "If the context is insufficient, just say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------\n",
    "## **Build the RetrievalQA Chain**\n",
    "\n",
    "### The `chain_type=\"stuff\"` combines all retrieved document chunks into the prompt.  \n",
    "### Other chain types (e.g., `map_reduce`, `refine`) can also be explored.\n",
    "\n",
    "### This chain utilizes the previously defined LLM and retriever.\n",
    "### It first retrieves relevant document chunks from the vector store and then passes them, along with the user query, to the chat-based LLM to generate an answer.\n",
    "\n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_chatgpt,\n",
    "    chain_type=\"stuff\",   # This chain type simply concatenates the retrieved documents for the LLM prompt.\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "\n",
    "# Define a convenience function to run queries on the RAG system.\n",
    "#\n",
    "# This function takes a text query, passes it to the RAG chain, and returns the answer.\n",
    "def query_rag_system(query: str) -> str:\n",
    "    \"\"\"Uses ChatGPT for generation with retrieved context. Returns the generated answer.\"\"\"\n",
    "    answer = rag_chain.run(query)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example biotech question\n",
    "question_1 = \"Summarize the latest CRISPR gene editing findings from these abstracts.\"\n",
    "answer_1 = rag_chain.run(question_1)\n",
    "print(f\"Q: {question_1}\\nA: {answer_1}\\n\")\n",
    "\n",
    "question_2 = \"Discuss current advancements in immunotherapy for lung cancer.\"\n",
    "answer_2 = rag_chain.run(question_2)\n",
    "print(f\"Q: {question_2}\\nA: {answer_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Display**\n",
    "### Function to display Q&A results in an easy-to-read format.\n",
    "### This is especially useful in a notebook setting to provide visual emphasis using increased font sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_qna(question, answer, q_font_size=18, a_font_size=16):\n",
    "    \"\"\"\n",
    "    Displays the question and answer with increased font sizes.\n",
    "    \"\"\"\n",
    "    html_content = f\"\"\"\n",
    "    <div st\n",
    "    yle=\"margin-bottom: 20px;\">\n",
    "        <p style=\"font-size:{q_font_size}px; font-weight: bold; color: #2E86C1;\">Question:</p>\n",
    "        <p style=\"font-size:{q_font_size}px; font-weight: bold; color: #2E86C1;\">{question}</p>\n",
    "        <p style=\"font-size:{a_font_size}px; margin-top: 10px;\"><strong>Answer:</strong> {answer}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Summarize recent findings on CRISPR gene editing.\"\n",
    "answer1 = query_rag_system(query1)\n",
    "display_qna(query1, answer1)\n",
    "\n",
    "query2 = \"What are the latest advancements in immunotherapy for cancer?\"\n",
    "answer2 = query_rag_system(query2)\n",
    "display_qna(query2, answer2)\n",
    "\n",
    "query3 = \"What are the latest EGFR mutation lung cancer therapies?\"\n",
    "answer3 = query_rag_system(query3)\n",
    "display_qna(query3, answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dynamic Index Updating**\n",
    "\n",
    "The RAG system leverages publicly available scientific abstracts from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) to retrieve and generate insightful responses. This could be extended to multiple datasets to enrich the RAG system's functionality. Extending to multiple datasets (e.g. [BioRxiv](https://www.biorxiv.org/)) involves similar steps: loading, embedding, and indexing. \n",
    "\n",
    "Below we merge embeddings from two new documents into the existing FAISS vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = [\n",
    "    \"New Abstract 1: Adenosine-to-inosine (A-to-I) editing of double-stranded RNA (dsRNA) by ADAR1 is an essential modifier of the immunogenicity of cellular dsRNA. The role of MDA5 in sensing unedited cellular dsRNA and the downstream activation of type I interferon (IFN) signaling are well established. However, we have an incomplete understanding of pathways that modify the response to unedited dsRNA. We performed a genome-wide CRISPR screen and showed that GGNBP2, CNOT10, and CNOT11 interact and regulate sensing of unedited cellular dsRNA. We found that GGNBP2 acts between dsRNA transcription and its cytoplasmic sensing by MDA5. GGNBP2 loss prevented induction of type I IFN and autoinflammation after the loss of ADAR1 editing activity by modifying the subcellular distribution of endogenous A-to-I editing substrates and reducing cytoplasmic dsRNA load. These findings reveal previously undescribed pathways to modify diseases associated with ADAR mutations and may be determinants of response or resistance to small-molecule ADAR1 inhibitors.\",\n",
    "    \"New Abstract 2: T cell immunoglobulin and mucin domain-containing protein 3 (TIM-3) is an important immune checkpoint molecule initially identified as a marker of IFN-Œ≥‚Äìproducing CD4+ and CD8+ T cells. Since then, our understanding of its role in immune responses has significantly expanded. Here, we review emerging evidence demonstrating unexpected roles for TIM-3 as a key regulator of myeloid cell function, in addition to recent work establishing TIM-3 as a delineator of terminal T cell exhaustion, thereby positioning TIM-3 at the interface between fatigued immune responses and reinvigoration. We share our perspective on the antagonism between TIM-3 and T cell stemness, discussing both cell-intrinsic and cell-extrinsic mechanisms underlying this relationship. Looking forward, we discuss approaches to decipher the underlying mechanisms by which TIM-3 regulates stemness, which has remarkable potential for the treatment of cancer, autoimmunity, and autoinflammation.\"\n",
    "]\n",
    "\n",
    "\n",
    "# 1) Convert raw strings into LangChain Document objects\n",
    "new_docs = [Document(page_content=text) for text in new_documents]\n",
    "\n",
    "# 2) Split the new documents for chunking (helps with retrieval accuracy)\n",
    "split_new_docs = splitter.split_documents(new_docs)\n",
    "\n",
    "# 3) Add these chunks to your vectorstore\n",
    "#    (This automatically handles embeddings and indexing behind the scenes.)\n",
    "vectorstore.add_documents(split_new_docs)\n",
    "\n",
    "# 4) Confirm the addition in a way specific to your vectorstore\n",
    "#    e.g., check how many docs are in the store\n",
    "print(\"Successfully added new documents to the vectorstore!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query1 = \"What are the roles of GGNBP2, CNOT10, and CNOT11 in regulating the sensing of unedited cellular dsRNA?\"\n",
    "new_answer1 = query_rag_system(new_query1)\n",
    "display_qna(new_query1, new_answer1)\n",
    "new_query2 = \"How does TIM-3 regulate myeloid cell activity and T cell exhaustion in immune responses?\"\n",
    "new_answer2 = query_rag_system(new_query2)\n",
    "\n",
    "display_qna(new_query2,new_answer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 3px solid white; width: 100%;\">\n",
    "<hr style=\"border: 3px solid white; width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Expert-Level Insights** <a id=\"expert-level-insights\"></a>\n",
    "\n",
    "###  **Challenges and Mitigation Strategies** <a id=\"challenges-and-mitigation-strategies\"></a>\n",
    "\n",
    "Building a robust RAG system involves navigating several challenges. Below are common issues and strategies to mitigate them:\n",
    "\n",
    "- **Retrieval Accuracy**:\n",
    "    - *Challenge*: Irrelevant documents may be retrieved.\n",
    "    - *Mitigation*: Use advanced embedding models and increase the quality of your dataset. Fine-tune the embedding model on domain-specific data to enhance relevance.\n",
    "    \n",
    "- **Hallucination in Generation**:\n",
    "    - *Challenge*: The generative model may produce incorrect or nonsensical answers.\n",
    "    - *Mitigation*: Fine-tune the generative model on domain-specific datasets and implement constraints during generation, such as length limits or specific tokens.\n",
    "    \n",
    "- **Scalability**:\n",
    "    - *Challenge*: Handling large datasets efficiently.\n",
    "    - *Mitigation*: Utilize optimized indexing techniques like FAISS's hierarchical navigable small world graphs (HNSW) for faster searches. Distribute the index across multiple machines if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 3px solid white; width: 100%;\">\n",
    "<hr style=\"border: 3px solid white; width: 100%;\">\n",
    "\n",
    "## **Conclusion** <a id=\"conclusion\"></a>\n",
    "\n",
    "In this tutorial, we've built a **Retrieval-Augmented Generation (RAG)** system tailored for the **biotechnology** and **biomedical research** industries. By integrating advanced retrieval mechanisms with powerful generative models, we've created a tool capable of navigating complex scientific data and providing insightful, evidence-based answers to domain-specific queries. This system showcases the potential of RAG in transforming how professionals interact with vast datasets, ultimately accelerating research and innovation in biotechnology and biomedical fields.\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "- #### **Chroma Vector Store:**\n",
    "  - We generated vector embeddings using OpenAI's embedding model and built a FAISS vector store.\n",
    "  - Employed **FAISS** for efficient similarity searches and vector storage within large datasets.\n",
    "  - Leveraged Chroma's capabilities to handle scalability and optimize performance for biomedical data.\n",
    "\n",
    "- #### **Generative Models:**\n",
    "  - Leveraged OpenAI models (4o-mini) for generating coherent and contextually relevant responses based on retrieved documents.\n",
    "  - Integrated the RAG chain architecture to mitigate hallucinations, ensuring that large language model (LLM) outputs remain evidence-based.\n",
    "\n",
    "- #### **Optimization:**\n",
    "  - Implemented chunking (splitting documents) to overcome token length limitations and ensure context continuity.\n",
    "\n",
    "- #### **Persistence:**\n",
    "  - Enabled saving and loading of Chroma vector stores and document lists to streamline future operations and support system scalability.\n",
    "\n",
    "### **Potential Next Steps:**\n",
    "\n",
    "- #### **Fine-Tuning Models:**\n",
    "  - Further enhance the system by fine-tuning the generative model on specialized biotechnology and biomedical datasets to improve domain-specific performance.\n",
    "\n",
    "- #### **Scalability:**\n",
    "  - Explore more advanced Chroma indexing techniques and alternative chain types to handle larger datasets and complex retrieval parameters.\n",
    "\n",
    "- #### **Deployment:**\n",
    "  - Consider deploying the RAG system as an API or integrating it into applications for real-time information retrieval and generation.\n",
    "  - Implement persistence strategies, such as saving/loading the vector store, to support scaling up the system for broader use cases.\n",
    "\n",
    "By following these next steps, you can continue to refine and expand the capabilities of your RAG system, ensuring it remains a valuable tool for advancing research and innovation in the biotechnology and biomedical research sectors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lab-learning-hub",
   "language": "python",
   "name": "ai-lab-learning-hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
